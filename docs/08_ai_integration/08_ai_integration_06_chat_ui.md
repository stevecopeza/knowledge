# AI Integration â€” Chat UI & RAG Flow

This document describes the implementation of the **Chat UI** and the underlying **RAG (Retrieval Augmented Generation)** flow.

---

## 1. Overview

The Chat UI allows administrators to ask natural language questions about the ingested knowledge base. It leverages the **Embedding Pipeline** to retrieve relevant context and the **Ollama** LLM to generate answers.

---

## 2. Architecture

### Frontend
- **Location**: `Knowledge > Ask AI` (Admin Menu)
- **Technology**: jQuery, standard WordPress Admin styles.
- **Interaction**:
    - User types a question.
    - User selects a **Search Mode** (Combined (RAG Prioritised), RAG Only, LLM Only, Combined (Balanced)).
    - AJAX request sends the question and mode to the backend.
    - UI shows a "Thinking..." state.
    - Answer is streamed or displayed upon completion (currently block response).
    - **Provenance Display**: Each answer displays the specific AI Provider and Model used (e.g., "Generated by: Local Server (llama3)").

### Backend (`ChatHandler`)
- **Endpoint**: `wp_ajax_knowledge_chat`
- **Process**:
    1.  **Mode Handling**: The system checks the requested `mode`.
    2.  **Embed Query**: (RAG/Combined only) The user's question is converted into a vector using `ProviderManager::embed()` (uses Primary Provider).
    3.  **Vector Search**: (RAG/Combined only) `VectorStore::search()` scans existing embeddings.
    4.  **Context Assembly**: The top 3 matching chunks are retrieved.
    5.  **Prompt Construction**: A specific prompt is selected based on the mode.
    6.  **Generation**: `ProviderManager::chat_with_failover()` is called to generate the final answer, attempting providers in priority order.

---

## 3. RAG Prompt Strategy

The system uses different prompts based on the selected mode:

### 3.1 Combined (RAG Prioritised) - Default
> "Answer the user's question using the provided context. If the context is insufficient, you may supplement with general knowledge to provide a complete answer, but prioritize the information from the knowledge base."

### 3.2 RAG Only (Strict)
> "Answer the user's question based ONLY on the provided context below. If the answer is not in the context, say you don't know."

### 3.3 LLM Only
> "Answer the user's question to the best of your ability using your general knowledge."

### 3.4 Combined (Balanced)
> "Answer the user's question using the provided context. If the context is insufficient, you may use your general knowledge to answer, but please mention if the information comes from outside the knowledge base."

---

## 4. Security & Permissions

- **Capability**: `manage_options` (Admins only for MVP).
- **Nonce Verification**: All AJAX requests are verified with `knowledge_chat_nonce`.
- **Sanitization**: User input is sanitized before processing.

---

## 5. Limitations (MVP)

- **Context Window**: Limited by the LLM's context window (e.g., 8k tokens for Llama 3).
- **History**: No multi-turn conversation memory (each question is independent).
- **Speed**: Dependent on local hardware (Ollama inference speed) and vector search size.
